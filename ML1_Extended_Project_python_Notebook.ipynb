{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**"
      ],
      "metadata": {
        "id": "GUr5SjuA2oQv"
      },
      "id": "GUr5SjuA2oQv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Context"
      ],
      "metadata": {
        "id": "3x2P_pQSFpQj"
      },
      "id": "3x2P_pQSFpQj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Car crashes are a leading cause of injury and death worldwide, and improving vehicle safety is a critical concern for car manufacturers. With advancements in technology and engineering, manufacturers are continuously seeking ways to design safer vehicles to reduce fatalities and severe injuries in the event of a crash. Despite these efforts, understanding the precise factors that contribute to survival in car crashes remains a complex challenge.\n",
        "\n",
        "The problem arises from the nature of car accidents, where various elements such as impact speed, the use of safety features, the type of collision, and the demographics of the occupants all play significant roles. Each crash is unique, and even minor variations can significantly affect the outcome for the occupants. This complexity necessitates a detailed analysis to identify which factors are most influential in determining survival outcomes.\n",
        "\n",
        "Solving this problem is essential for several reasons:\n",
        "\n",
        "1. Safety Regulations\n",
        "2. Design Improvements\n",
        "3. Public Health\n",
        "4. Consumer Confidence"
      ],
      "metadata": {
        "id": "vn7-PjjJFl0d"
      },
      "id": "vn7-PjjJFl0d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objective"
      ],
      "metadata": {
        "id": "ukyDOAPeFrc6"
      },
      "id": "ukyDOAPeFrc6"
    },
    {
      "cell_type": "markdown",
      "id": "dense-medicaid",
      "metadata": {
        "id": "dense-medicaid"
      },
      "source": [
        "\n",
        "Over the last year, the Department of Road Transport has witnessed a 15% YoY rise in the number of car crashes happening in urban areas. While they have the causes of the accidents post-facto, they want to preempt the risk to increase road safety.\n",
        "\n",
        "You have been hired as a data scientist and provided with a sample of the historical car crashes over 5 years, with different attributes of the car and the occupant relevant to the car crash. Your objective is to analyze the data, identify patterns in car crashes, build a predictive model to determine the likelihood of survival in car crashes based on the factors and identify the most critical factors that influence survival outcomes, thereby helping the department come up with necessary safety regulations that must be adopted by all vehicle manufacturers and users."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Description"
      ],
      "metadata": {
        "id": "wTMkO7AoF2Mc"
      },
      "id": "wTMkO7AoF2Mc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data contains the different attributes of car crashes, with the outcome variable being whether the occupant was deceased during the crash or not. The detailed data dictionary is given below."
      ],
      "metadata": {
        "id": "4oNKoQ-YF5J9"
      },
      "id": "4oNKoQ-YF5J9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Dictionary**\n",
        "\n",
        "* caseid: character, created by pasting together the population sampling unit, the case number, and the vehicle number. Within each year, use this to uniquely identify the vehicle.\n",
        "* speed_range: factor with levels (estimated impact speeds) 1-9 km/h, 10-24 km/h, 25-39 km/h, 40-54 km/h, 55+ km/h\n",
        "*  weight: Observation weights, albeit of uncertain accuracy, are designed to account for varying sampling probabilities. (The inverse probability weighting estimator can be used to demonstrate causality when the researcher cannot conduct a controlled experiment but has observed data to model)\n",
        "* seatbelt: a factor with levels none or belted\n",
        "* frontal_impact: a numeric vector; 0 = non-frontal, 1=frontal impact\n",
        "* sex: a factor with levels f: Female or m: Male\n",
        "* age_of_occ: age of occupant in years\n",
        "* year_of_acc: year of accident\n",
        "* model_year: Year of model of vehicle; a numeric vector\n",
        "* airbag: Did one or more (driver or passenger) airbag(s) deploy? This factor has levels deploy, nodeploy, and unavail\n",
        "* occ_role: a factor with levels driver or pass: passenger\n",
        "* deceased: the target variable with levels no (survived) or yes (not survived / deceased)\n"
      ],
      "metadata": {
        "id": "qbYarcjVFu0P"
      },
      "id": "qbYarcjVFu0P"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Please read the instructions carefully before starting the project.**"
      ],
      "metadata": {
        "id": "AoJq5iPR5dAg"
      },
      "id": "AoJq5iPR5dAg"
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a commented Python Notebook file in which all the instructions and tasks to be performed are mentioned.\n",
        "* Blanks '_______' are provided in the notebook that\n",
        "needs to be filled with an appropriate code to get the correct result. With every '_______' blank, there is a comment that briefly describes what needs to be filled in the blank space.\n",
        "* Identify the task to be performed correctly, and only then proceed to write the required code.\n",
        "* Fill the code wherever asked by the commented lines like \"# write your code here\" or \"# complete the code\". Running incomplete code may throw error.\n",
        "* Please run the codes in a sequential manner from the beginning to avoid any unnecessary errors.\n",
        "* Add the results/observations (wherever mentioned) derived from the analysis in the presentation and submit the same."
      ],
      "metadata": {
        "id": "phTUEO3UKbro"
      },
      "id": "phTUEO3UKbro"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing the necessary libraries**"
      ],
      "metadata": {
        "id": "iLjl7RE-2rKR"
      },
      "id": "iLjl7RE-2rKR"
    },
    {
      "cell_type": "code",
      "source": [
        "# Libraries to help with reading and manipulating data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# libaries to help with data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Removes the limit for the number of displayed columns\n",
        "pd.set_option(\"display.max_columns\", None)\n",
        "# Sets the limit for the number of displayed rows\n",
        "pd.set_option(\"display.max_rows\", 200)\n",
        "# setting the precision of floating numbers to 5 decimal points\n",
        "pd.set_option(\"display.float_format\", lambda x: \"%.5f\" % x)\n",
        "\n",
        "# Library to split data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# To build model for prediction\n",
        "import statsmodels.api as SM\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import tree\n",
        "\n",
        "# To tune different models\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# To get diferent metric scores\n",
        "from sklearn.metrics import (\n",
        "    f1_score,\n",
        "    accuracy_score,\n",
        "    recall_score,\n",
        "    precision_score,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    precision_recall_curve,\n",
        "    roc_curve\n",
        ")\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "e0Kh2lF72uKX"
      },
      "id": "e0Kh2lF72uKX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fantastic-rebel",
      "metadata": {
        "id": "fantastic-rebel"
      },
      "source": [
        "# **Loading the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment and run the following lines for Google Colab\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "jBc32pyEthbM"
      },
      "id": "jBc32pyEthbM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "precious-leonard",
      "metadata": {
        "id": "precious-leonard"
      },
      "outputs": [],
      "source": [
        "cars = pd.read_csv('_____')    ##  complete the code to read the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "geographic-gender",
      "metadata": {
        "id": "geographic-gender"
      },
      "outputs": [],
      "source": [
        "# copying data to another variable to avoid any changes to original data\n",
        "data = cars.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Overview**"
      ],
      "metadata": {
        "id": "eJut93Kd2-gR"
      },
      "id": "eJut93Kd2-gR"
    },
    {
      "cell_type": "markdown",
      "id": "convinced-blackberry",
      "metadata": {
        "id": "convinced-blackberry"
      },
      "source": [
        "### View the first and last 5 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tested-adjustment",
      "metadata": {
        "id": "tested-adjustment"
      },
      "outputs": [],
      "source": [
        "data._______ ##  Complete the code to view top 5 rows of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "demonstrated-charger",
      "metadata": {
        "id": "demonstrated-charger"
      },
      "outputs": [],
      "source": [
        "data._______ ##  Complete the code to view last 5 rows of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "prepared-clause",
      "metadata": {
        "id": "prepared-clause"
      },
      "source": [
        "### Understand the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "likely-scene",
      "metadata": {
        "id": "likely-scene"
      },
      "outputs": [],
      "source": [
        "data._______ ##  Complete the code to view dimensions of the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "creative-warner",
      "metadata": {
        "id": "creative-warner"
      },
      "source": [
        "### Check the data types of the columns for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "expanded-technique",
      "metadata": {
        "id": "expanded-technique"
      },
      "outputs": [],
      "source": [
        "data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Statistical summary of the dataset"
      ],
      "metadata": {
        "id": "dbGZU90ThWsR"
      },
      "id": "dbGZU90ThWsR"
    },
    {
      "cell_type": "code",
      "source": [
        "data._______ ##  Complete the code to view the statistical summary of the data"
      ],
      "metadata": {
        "id": "vv28bixVHf5V"
      },
      "id": "vv28bixVHf5V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for duplicate values"
      ],
      "metadata": {
        "id": "98h5fWKJ3MUR"
      },
      "id": "98h5fWKJ3MUR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "greenhouse-vertical",
      "metadata": {
        "id": "greenhouse-vertical"
      },
      "outputs": [],
      "source": [
        "# checking for duplicate values\n",
        "data._______ ##  Complete the code to check duplicate entries in the data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for missing values"
      ],
      "metadata": {
        "id": "Mwe5Ukj9eb-E"
      },
      "id": "Mwe5Ukj9eb-E"
    },
    {
      "cell_type": "code",
      "source": [
        "data._______ ##  Complete the code to view the missing values in the dataset"
      ],
      "metadata": {
        "id": "SGy6wR30eb-K"
      },
      "execution_count": null,
      "outputs": [],
      "id": "SGy6wR30eb-K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating the veh_usage_duration variable"
      ],
      "metadata": {
        "id": "Dss3NAdBRwzA"
      },
      "id": "Dss3NAdBRwzA"
    },
    {
      "cell_type": "markdown",
      "source": [
        "* veh_usage_duration: Indicates the time period (in years) the vehicle has been in use"
      ],
      "metadata": {
        "id": "unBxnHGUSWy7"
      },
      "id": "unBxnHGUSWy7"
    },
    {
      "cell_type": "code",
      "source": [
        "data['veh_usage_duration'] = data['year_of_acc'] - data['model_year']"
      ],
      "metadata": {
        "id": "-2iMuud4S20u"
      },
      "id": "-2iMuud4S20u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "realistic-mortgage",
      "metadata": {
        "id": "realistic-mortgage"
      },
      "source": [
        "# **Exploratory Data Analysis (EDA)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for EDA"
      ],
      "metadata": {
        "id": "vWwP3_ijOXY2"
      },
      "id": "vWwP3_ijOXY2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The below functions need to be defined to carry out the EDA.**"
      ],
      "metadata": {
        "id": "ZTsE-NT6_L6o"
      },
      "id": "ZTsE-NT6_L6o"
    },
    {
      "cell_type": "code",
      "source": [
        "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (15,10))\n",
        "    kde: whether to show the density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram"
      ],
      "metadata": {
        "id": "75zDBkVj_LKQ"
      },
      "id": "75zDBkVj_LKQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to create labeled barplots\n",
        "\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 2, 6))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 2, 6))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        hue=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n],\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ],
      "metadata": {
        "id": "3Ayf3YwD_O4n"
      },
      "id": "3Ayf3YwD_O4n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\", frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "H83OecPd_SR-"
      },
      "id": "H83OecPd_SR-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### function to plot distributions wrt target\n",
        "\n",
        "\n",
        "def distribution_plot_wrt_target(data, predictor, target):\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    target_uniq = data[target].unique()\n",
        "\n",
        "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[0]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 0],\n",
        "        color=\"teal\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[1]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 1],\n",
        "        color=\"orange\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
        "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
        "\n",
        "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
        "    sns.boxplot(\n",
        "        data=data,\n",
        "        x=target,\n",
        "        y=predictor,\n",
        "        ax=axs[1, 1],\n",
        "        showfliers=False,\n",
        "        palette=\"gist_rainbow\",\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T1wvTkHa_UwP"
      },
      "id": "T1wvTkHa_UwP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "arbitrary-intelligence",
      "metadata": {
        "id": "arbitrary-intelligence"
      },
      "source": [
        "### Univariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "celtic-florist"
      },
      "source": [
        "#### Observations on deceased"
      ],
      "id": "celtic-florist"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "finite-kingston"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data, \"deceased\", perc=True)  ##Complete the code to get the labeled_barplot for deceased"
      ],
      "id": "finite-kingston"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Gr7JKZoV23-"
      },
      "source": [
        "#### Observations on weight"
      ],
      "id": "5Gr7JKZoV23-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9MiIJrP_V23-"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(data, \"weight\")"
      ],
      "id": "9MiIJrP_V23-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "013cnPOcV23-"
      },
      "source": [
        "#### Observations on age_of_occ"
      ],
      "id": "013cnPOcV23-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mp2WcmHV23-"
      },
      "outputs": [],
      "source": [
        "histogram_boxplot(data, \"_______\")  ##Complete the code to get the histogram_boxplot for age_of_occ"
      ],
      "id": "5mp2WcmHV23-"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1W4vkEjBV23_"
      },
      "source": [
        "#### Observations on speed_range"
      ],
      "id": "1W4vkEjBV23_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "israeli-sympathy"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data, \"_______\", perc=True)  ##Complete the code to get the labeled_barplot for speed_range"
      ],
      "id": "israeli-sympathy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "affiliated-accreditation"
      },
      "source": [
        "#### Observations on airbag"
      ],
      "id": "affiliated-accreditation"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "great-kitchen"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data, \"_______\", perc=True)  ##Complete the code to get the labeled_barplot for airbag"
      ],
      "id": "great-kitchen"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "thermal-resource"
      },
      "source": [
        "#### Observations on seatbelt"
      ],
      "id": "thermal-resource"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": false,
        "id": "bizarre-serbia"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data, \"_______\", perc=True)  ##Complete the code to get the labeled_barplot for seatbelt"
      ],
      "id": "bizarre-serbia"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "greenhouse-regression"
      },
      "source": [
        "#### Observations on frontal_impact"
      ],
      "id": "greenhouse-regression"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "handy-talent"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data, \"_______\", perc=True)  ##Complete the code to get the labeled_barplot for frontal_impact"
      ],
      "id": "handy-talent"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "southeast-avenue"
      },
      "source": [
        "#### Observations on sex"
      ],
      "id": "southeast-avenue"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "retired-preliminary"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(data, \"_______\", perc=True)  ##Complete the code to get the labeled_barplot for sex"
      ],
      "id": "retired-preliminary"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations on model_year"
      ],
      "metadata": {
        "id": "4kN-mnIoPssO"
      },
      "id": "4kN-mnIoPssO"
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(data, \"_______\")  ##Complete the code to get the histogram_boxplot for model_year"
      ],
      "metadata": {
        "id": "kQ8buTZDPwqH"
      },
      "id": "kQ8buTZDPwqH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations on occ_role"
      ],
      "metadata": {
        "id": "tyaQ1horP58t"
      },
      "id": "tyaQ1horP58t"
    },
    {
      "cell_type": "code",
      "source": [
        "labeled_barplot(data, \"_______\", perc=True)  ##Complete the code to get the labeled_barplot for occ_role"
      ],
      "metadata": {
        "id": "RhBeUxdFP9cE"
      },
      "id": "RhBeUxdFP9cE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  Observations on veh_usage_duration"
      ],
      "metadata": {
        "id": "7a8KHH4FTNLw"
      },
      "id": "7a8KHH4FTNLw"
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_boxplot(data, \"_______\")  ##Complete the code to get the histogram_boxplot for veh_usage_duration"
      ],
      "metadata": {
        "id": "7xAoow_1TdoH"
      },
      "id": "7xAoow_1TdoH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMH05ybiV24C"
      },
      "source": [
        "### Bivariate Analysis"
      ],
      "id": "tMH05ybiV24C"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q8mW5GGX6w1c"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data, \"speed_range\", \"deceased\")"
      ],
      "id": "Q8mW5GGX6w1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1_It2G036w1c"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data, \"_______\", \"_______\") ## Complete the code to get stacked_barplot for seatbelt and deceased"
      ],
      "id": "1_It2G036w1c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_XHcrC8ascKg"
      },
      "outputs": [],
      "source": [
        "stacked_barplot(data, \"_______\", \"_______\")  ## Complete the code to get stacked_barplot for frontal_impact and deceased"
      ],
      "id": "_XHcrC8ascKg"
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(data, \"_______\", \"_______\")  ## Complete the code to get stacked_barplot for sex and deceased"
      ],
      "metadata": {
        "id": "z8bsopXu7KP0"
      },
      "id": "z8bsopXu7KP0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(data, \"_______\", \"_______\")  ## Complete the code to get stacked_barplot for airbag and deceased"
      ],
      "metadata": {
        "id": "0VAqgAqu7NCE"
      },
      "id": "0VAqgAqu7NCE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stacked_barplot(data, \"_______\", \"_______\")  ## Complete the code to get stacked_barplot for occ_role and deceased"
      ],
      "metadata": {
        "id": "A6-o6aRk9U3l"
      },
      "id": "A6-o6aRk9U3l",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC42Wy1t6w1d"
      },
      "outputs": [],
      "source": [
        "distribution_plot_wrt_target(data, \"age_of_occ\", \"deceased\")"
      ],
      "id": "qC42Wy1t6w1d"
    },
    {
      "cell_type": "code",
      "source": [
        "distribution_plot_wrt_target(data, \"_______\", \"_______\")  ## Complete the code to get distribution_plot_wrt_target for veh_usage_duration and deceased"
      ],
      "metadata": {
        "id": "x4nNEybX9pkW"
      },
      "id": "x4nNEybX9pkW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cols_list = [\"weight\", \"age_of_occ\", \"veh_usage_duration\"]\n",
        "\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.heatmap(\n",
        "    data[cols_list].corr(), annot=True, vmin=-1, vmax=1, fmt=\".2f\", cmap=\"Spectral\"\n",
        ")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_fnzt9Ey0XXb"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_fnzt9Ey0XXb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "_Y8TLuicA7DI"
      },
      "id": "_Y8TLuicA7DI"
    },
    {
      "cell_type": "markdown",
      "id": "powerful-couple",
      "metadata": {
        "id": "powerful-couple"
      },
      "source": [
        "### Outlier Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imported-uganda",
      "metadata": {
        "id": "imported-uganda"
      },
      "outputs": [],
      "source": [
        "# outlier detection using boxplot\n",
        "numeric_columns = data.select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "for i, variable in enumerate(numeric_columns):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.boxplot(data[variable], whis=1.5)\n",
        "    plt.tight_layout()\n",
        "    plt.title(variable)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pleased-chicken",
      "metadata": {
        "id": "pleased-chicken"
      },
      "source": [
        "### Data Preparation for modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's drop the unnecessary columns first before we proceed forward**."
      ],
      "metadata": {
        "id": "hrQbHcQBRuJ-"
      },
      "id": "hrQbHcQBRuJ-"
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(['_______', \"_______\", \"_______\"], axis = 1, inplace = True)  ## Complete the code to drop the unnecessary columns (caseid, year_of_acc, and model_year)"
      ],
      "metadata": {
        "id": "gFrvrR8Nu_lF"
      },
      "id": "gFrvrR8Nu_lF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "oslhCarwvTMw"
      },
      "id": "oslhCarwvTMw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[\"deceased\"].replace({\"no\":0, \"yes\":1},inplace=True)"
      ],
      "metadata": {
        "id": "3qEBVhEovdjO"
      },
      "id": "3qEBVhEovdjO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "durable-allergy",
      "metadata": {
        "id": "durable-allergy"
      },
      "outputs": [],
      "source": [
        "X = data.drop([\"deceased\"], axis=1)\n",
        "Y = data[\"deceased\"]\n",
        "\n",
        "X = pd.get_dummies(X, drop_first=True)\n",
        "\n",
        "X = X.astype(float)\n",
        "\n",
        "# Splitting data in train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, Y, test_size=_______, random_state=42           ## Complete the code to split the data in 70:30 ratio\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.reset_index(inplace = True, drop = True)"
      ],
      "metadata": {
        "id": "WTz4_OJ700jt"
      },
      "id": "WTz4_OJ700jt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ecological-principal",
      "metadata": {
        "id": "ecological-principal"
      },
      "outputs": [],
      "source": [
        "print(\"Shape of Training set : \", X_train.shape)\n",
        "print(\"Shape of test set : \", X_test.shape)\n",
        "print(\"Shape of Training set : \", y_train.shape)\n",
        "print(\"Shape of test set : \", y_test.shape)\n",
        "print(\"Percentage of classes in training set:\")\n",
        "print(y_train.value_counts(normalize=True))\n",
        "print(\"Percentage of classes in test set:\")\n",
        "print(y_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Scaling the Data"
      ],
      "metadata": {
        "id": "SpNXlnOyJv1t"
      },
      "id": "SpNXlnOyJv1t"
    },
    {
      "cell_type": "code",
      "source": [
        "sc = StandardScaler()\n",
        "\n",
        "X_train_scaled = pd.DataFrame(sc.fit_transform(X_train), columns=X_train.columns)\n",
        "X_test_scaled = pd.DataFrame(sc.transform(X_test), columns=X_test.columns)"
      ],
      "metadata": {
        "id": "bkh5kSA_JvAr"
      },
      "id": "bkh5kSA_JvAr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Building**"
      ],
      "metadata": {
        "id": "NlsnzCLqBIyV"
      },
      "id": "NlsnzCLqBIyV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation criterion"
      ],
      "metadata": {
        "id": "mrSbf72Oqpd9"
      },
      "id": "mrSbf72Oqpd9"
    },
    {
      "cell_type": "markdown",
      "id": "cultural-engagement",
      "metadata": {
        "id": "cultural-engagement"
      },
      "source": [
        "*  "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ruled-appointment",
      "metadata": {
        "id": "ruled-appointment"
      },
      "source": [
        "First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n",
        "* The model_performance_classification_sklearn function will be used to check the model performance of models.\n",
        "* The confusion_matrix_sklearn function will be used to plot the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
        "def model_performance_classification(model, predictors, target, threshold = 0.5):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    prob_pred = model.predict(predictors)\n",
        "    class_pred = [1 if i >= threshold else 0 for i in prob_pred]\n",
        "\n",
        "    acc = accuracy_score(target, class_pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, class_pred)  # to compute Recall\n",
        "    precision = precision_score(target, class_pred)  # to compute Precision\n",
        "    f1 = f1_score(target, class_pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ],
      "metadata": {
        "id": "1DRYfIeXR7n_"
      },
      "id": "1DRYfIeXR7n_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(model, predictors, target, threshold = 0.5):\n",
        "    \"\"\"\n",
        "    To plot the confusion_matrix with percentages\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "    prob_pred = model.predict(predictors)\n",
        "    class_pred = [1 if i >= threshold else 0 for i in prob_pred]\n",
        "    cm = confusion_matrix(target, class_pred)\n",
        "    labels = np.asarray(\n",
        "        [\n",
        "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
        "            for item in cm.flatten()\n",
        "        ]\n",
        "    ).reshape(2, 2)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ],
      "metadata": {
        "id": "EBvJebPJR7rG"
      },
      "id": "EBvJebPJR7rG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression (with Statsmodel)"
      ],
      "metadata": {
        "id": "YwcKKDSyRRWz"
      },
      "id": "YwcKKDSyRRWz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding constant to data for Logistic Regression\n",
        "X_train_with_intercept = SM.add_constant(X_train_scaled)\n",
        "X_test_with_intercept = SM.add_constant(X_test_scaled)"
      ],
      "metadata": {
        "id": "WS4CeKOBRc0E"
      },
      "id": "WS4CeKOBRc0E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_with_intercept.head()"
      ],
      "metadata": {
        "id": "z_auiiZ4SSkH"
      },
      "id": "z_auiiZ4SSkH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LogisticReg = SM.Logit(y_train, X_train_with_intercept).fit()\n",
        "print(LogisticReg.summary())"
      ],
      "metadata": {
        "id": "IgNssznERash"
      },
      "id": "IgNssznERash",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Logistic Regression model performance on training set"
      ],
      "metadata": {
        "id": "yHqVXLXkVwbj"
      },
      "id": "yHqVXLXkVwbj"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = LogisticReg.predict(X_train_with_intercept)\n",
        "y_pred.head()"
      ],
      "metadata": {
        "id": "2GS39cY6UeBP"
      },
      "id": "2GS39cY6UeBP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_reg_perf_train = model_performance_classification(\n",
        "    LogisticReg, X_train_with_intercept, y_train\n",
        ")\n",
        "logistic_reg_perf_train"
      ],
      "metadata": {
        "id": "7k2yr5PVR7t-"
      },
      "id": "7k2yr5PVR7t-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(LogisticReg, X_train_with_intercept, y_train)"
      ],
      "metadata": {
        "id": "w0CSx_V2R7xG"
      },
      "id": "w0CSx_V2R7xG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Logistic Regression model performance on test set"
      ],
      "metadata": {
        "id": "vyhkBZ3MWHNC"
      },
      "id": "vyhkBZ3MWHNC"
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_reg_perf_test = model_performance_classification(\n",
        "    LogisticReg, X_test_with_intercept, y_test\n",
        ")\n",
        "logistic_reg_perf_test"
      ],
      "metadata": {
        "id": "U6WrmBzEWFBA"
      },
      "id": "U6WrmBzEWFBA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(LogisticReg, X_test_with_intercept, y_test)"
      ],
      "metadata": {
        "id": "mTsGYFL_WFET"
      },
      "id": "mTsGYFL_WFET",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Naive - Bayes Classifier"
      ],
      "metadata": {
        "id": "E4yMf6psT619"
      },
      "id": "E4yMf6psT619"
    },
    {
      "cell_type": "code",
      "source": [
        "#Build Naive Bayes Model\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "cRZ_MU0PYVMV"
      },
      "id": "cRZ_MU0PYVMV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Naive - Bayes Classifier performance on training set"
      ],
      "metadata": {
        "id": "PE3438sBzKJr"
      },
      "id": "PE3438sBzKJr"
    },
    {
      "cell_type": "code",
      "source": [
        "nb_perf_train = model_performance_classification(_______)  ## Complete the code to get the model performance on training set\n",
        "nb_perf_train"
      ],
      "metadata": {
        "id": "6G77dj1WYWjd"
      },
      "id": "6G77dj1WYWjd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to plot the confusion matrix for training set"
      ],
      "metadata": {
        "id": "RTwjvOkH0KD0"
      },
      "id": "RTwjvOkH0KD0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking Naive - Bayes Classifier performance on test set"
      ],
      "metadata": {
        "id": "fgpGGT8F0HOI"
      },
      "id": "fgpGGT8F0HOI"
    },
    {
      "cell_type": "code",
      "source": [
        "nb_perf_test = model_performance_classification(_______)  ## Complete the code to get the model performance on test set\n",
        "nb_perf_test"
      ],
      "metadata": {
        "id": "vLpf0j8PYbU4"
      },
      "id": "vLpf0j8PYbU4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to plot the confusion matrix for test set"
      ],
      "metadata": {
        "id": "Wq-8kZou0nMs"
      },
      "id": "Wq-8kZou0nMs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN Classifier (K = 3)"
      ],
      "metadata": {
        "id": "xWZ3-XPnlM2V"
      },
      "id": "xWZ3-XPnlM2V"
    },
    {
      "cell_type": "code",
      "source": [
        "#Build KNN Model\n",
        "knn_model = KNeighborsClassifier(n_neighbors = _______)  ## Complete the code to build KNN model with nummber of neighbors as 3\n",
        "knn_model.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "-zl14XVAlM2V"
      },
      "execution_count": null,
      "outputs": [],
      "id": "-zl14XVAlM2V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking KNN Classifier performance on training set"
      ],
      "metadata": {
        "id": "iGrBce-blM2W"
      },
      "id": "iGrBce-blM2W"
    },
    {
      "cell_type": "code",
      "source": [
        "knn_perf_train = model_performance_classification(_______)  ## Complete the code to get the model performance on training set\n",
        "knn_perf_train"
      ],
      "metadata": {
        "id": "aHdBnr7BlM2W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aHdBnr7BlM2W"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to plot the confusion matrix for training set"
      ],
      "metadata": {
        "id": "h9ueTuxalM2W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "h9ueTuxalM2W"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking KNN Classifier performance on test set"
      ],
      "metadata": {
        "id": "XHqjLE38lM2W"
      },
      "id": "XHqjLE38lM2W"
    },
    {
      "cell_type": "code",
      "source": [
        "knn_perf_test = model_performance_classification(_______)  ## Complete the code to get the model performance on test set\n",
        "knn_perf_test"
      ],
      "metadata": {
        "id": "HZo-vl_7lM2W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "HZo-vl_7lM2W"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to plot the confusion matrix for test set"
      ],
      "metadata": {
        "id": "L_sWKfgelM2W"
      },
      "execution_count": null,
      "outputs": [],
      "id": "L_sWKfgelM2W"
    },
    {
      "cell_type": "markdown",
      "id": "separated-prague",
      "metadata": {
        "id": "separated-prague"
      },
      "source": [
        "## Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "GK3Q3MgFI7ER"
      },
      "id": "GK3Q3MgFI7ER",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "identified-upper",
      "metadata": {
        "id": "identified-upper"
      },
      "source": [
        "### Checking Decision Tree Classifier performance on training set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "falling-squad",
      "metadata": {
        "scrolled": true,
        "id": "falling-squad"
      },
      "outputs": [],
      "source": [
        "decision_tree_perf_train = model_performance_classification(_______)  ## Complete the code to get the model performance on training set\n",
        "decision_tree_perf_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "female-kennedy",
      "metadata": {
        "id": "female-kennedy"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to plot the confusion matrix for training set"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "neither-omaha",
      "metadata": {
        "id": "neither-omaha"
      },
      "source": [
        "### Checking Decision Tree Classifier performance on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "applied-magazine",
      "metadata": {
        "scrolled": true,
        "id": "applied-magazine"
      },
      "outputs": [],
      "source": [
        "decision_tree_perf_test = model_performance_classification(_______)  ## Complete the code to get the model performance on test set\n",
        "decision_tree_perf_test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to plot the confusion matrix for test set"
      ],
      "metadata": {
        "id": "3FNLxNhP3cZd"
      },
      "id": "3FNLxNhP3cZd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Performance Improvement**"
      ],
      "metadata": {
        "id": "FGc93_PhaS6c"
      },
      "id": "FGc93_PhaS6c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistic Regression (deal with high p-value variables and determine optimal threshold using ROC curve)"
      ],
      "metadata": {
        "id": "G2YYO94vahF3"
      },
      "id": "G2YYO94vahF3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dealing with high p-value variables"
      ],
      "metadata": {
        "id": "bBr7Ho4pEDYJ"
      },
      "id": "bBr7Ho4pEDYJ"
    },
    {
      "cell_type": "code",
      "source": [
        "# initial list of columns\n",
        "predictors = X_train_with_intercept.copy()\n",
        "cols = predictors.columns.tolist()\n",
        "\n",
        "# setting an initial max p-value\n",
        "max_p_value = 1\n",
        "\n",
        "while len(cols) > 0:\n",
        "    # defining the train set\n",
        "    x_train_aux = predictors[cols]\n",
        "\n",
        "    # fitting the model\n",
        "    model = SM.Logit(y_train, x_train_aux).fit()\n",
        "\n",
        "    # getting the p-values and the maximum p-value\n",
        "    p_values = model.pvalues\n",
        "    max_p_value = max(p_values)\n",
        "\n",
        "    # name of the variable with maximum p-value\n",
        "    feature_with_p_max = p_values.idxmax()\n",
        "\n",
        "    if max_p_value > 0.05:\n",
        "        cols.remove(feature_with_p_max)\n",
        "        print(f\"Dropping column {feature_with_p_max} with p-value: {max_p_value}\")\n",
        "    else:\n",
        "        break\n",
        "\n",
        "selected_features = cols\n",
        "print(selected_features)"
      ],
      "metadata": {
        "id": "EJxYNyN5_cM7"
      },
      "id": "EJxYNyN5_cM7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_significant = X_train_with_intercept[selected_features]\n",
        "X_test_significant = X_test_with_intercept[_______]  ## Complete the code to get the test set with significant features\n",
        "X_train_significant.head(10)"
      ],
      "metadata": {
        "id": "aDRNteyIHcOC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "aDRNteyIHcOC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Logistic Regression model again with only the significant features"
      ],
      "metadata": {
        "id": "4Ug4i4PeftHw"
      },
      "id": "4Ug4i4PeftHw"
    },
    {
      "cell_type": "code",
      "source": [
        "LogisticReg_2 = SM.Logit(y_train, _______).fit()  ## Complete the code to train the Logistic Regression model with significant features\n",
        "print(LogisticReg_2.summary())"
      ],
      "metadata": {
        "id": "tE8Uved7Ky7h"
      },
      "execution_count": null,
      "outputs": [],
      "id": "tE8Uved7Ky7h"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Determining optimal threshold using ROC Curve"
      ],
      "metadata": {
        "id": "yjY9fDUnbwSr"
      },
      "id": "yjY9fDUnbwSr"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = LogisticReg_2.predict(X_train_significant)\n",
        "fpr, tpr, thresholds = roc_curve(y_train, y_pred)\n",
        "\n",
        "# Plot ROC curve\n",
        "roc_auc = _______(y_train, y_pred)  ## Complete the code to get the ROC-AUC score\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1XTnwrUvDO56"
      },
      "execution_count": null,
      "outputs": [],
      "id": "1XTnwrUvDO56"
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the optimal threshold\n",
        "optimal_idx = np.argmax(tpr - fpr)\n",
        "optimal_threshold_logit = round(thresholds[optimal_idx], 3)\n",
        "print(\"\\nOptimal Threshold: \", optimal_threshold_logit)"
      ],
      "metadata": {
        "id": "N-v3aLXaDTtC"
      },
      "execution_count": null,
      "outputs": [],
      "id": "N-v3aLXaDTtC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking new Logistic Regression model performance on training set"
      ],
      "metadata": {
        "id": "C2BnozY0fGOD"
      },
      "id": "C2BnozY0fGOD"
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_reg_new_perf_train = model_performance_classification(\n",
        "    LogisticReg_2, X_train_significant, y_train, optimal_threshold_logit\n",
        ")\n",
        "logistic_reg_new_perf_train"
      ],
      "metadata": {
        "id": "_jqjJyIeby_L"
      },
      "execution_count": null,
      "outputs": [],
      "id": "_jqjJyIeby_L"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(LogisticReg_2, X_train_significant, y_train, optimal_threshold_logit)"
      ],
      "metadata": {
        "id": "zJy21rnlbzCU"
      },
      "execution_count": null,
      "outputs": [],
      "id": "zJy21rnlbzCU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking tuned Logistic Regression model performance on test set"
      ],
      "metadata": {
        "id": "RtWQWYo6fmJc"
      },
      "id": "RtWQWYo6fmJc"
    },
    {
      "cell_type": "code",
      "source": [
        "logistic_reg_new_perf_test = model_performance_classification(\n",
        "    LogisticReg_2, X_test_significant, y_test, optimal_threshold_logit\n",
        ")\n",
        "\n",
        "logistic_reg_new_perf_test"
      ],
      "metadata": {
        "id": "FYDTzmFYbzFw"
      },
      "execution_count": null,
      "outputs": [],
      "id": "FYDTzmFYbzFw"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(LogisticReg_2, X_test_significant, y_test, optimal_threshold_logit)"
      ],
      "metadata": {
        "id": "UW19u2JWfi7K"
      },
      "execution_count": null,
      "outputs": [],
      "id": "UW19u2JWfi7K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN Classifier (different values of K)"
      ],
      "metadata": {
        "id": "nOY_a412al9a"
      },
      "id": "nOY_a412al9a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### KNN Classifier Performance Improvement using different k values"
      ],
      "metadata": {
        "id": "iDfbR4XDE_-s"
      },
      "id": "iDfbR4XDE_-s"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the range for k values\n",
        "k_values = range(_______)  ## Complete the code to define the range for k-values between 2 and 20 (both inclusive)\n",
        "\n",
        "# Initialize variables to store the best k and the highest recall score\n",
        "best_k = 0\n",
        "best_recall = 0\n",
        "\n",
        "# Loop through each k value\n",
        "for k in k_values:\n",
        "    # Create and fit the KNN classifier with the current k value\n",
        "    knn = KNeighborsClassifier(n_neighbors = _______)  ## Complete the code to build KNN model with nummber of neighbors as k in each iteration\n",
        "    knn.fit(X_train_scaled, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = knn.predict(X_train_scaled)\n",
        "\n",
        "    # Calculate the recall score\n",
        "    recall = recall_score(y_train, y_pred)\n",
        "\n",
        "    # Print the recall score for the current k value\n",
        "    print(f'Recall for k={k}: {recall}')\n",
        "\n",
        "    # Update the best k and best recall score if the current recall is higher\n",
        "    if recall > best_recall:\n",
        "        best_recall = recall\n",
        "        best_k = k\n",
        "\n",
        "# Print the best k value and its recall score\n",
        "print(f'\\nThe best value of k is: {best_k} with a recall of: {best_recall}')"
      ],
      "metadata": {
        "id": "bgaBtMsfTxdp"
      },
      "execution_count": null,
      "outputs": [],
      "id": "bgaBtMsfTxdp"
    },
    {
      "cell_type": "code",
      "source": [
        "knn_tuned = KNeighborsClassifier(n_neighbors = _______)  ## Complete the code to build KNN model with nummber of neighbors as best_k\n",
        "knn_tuned.fit(X_train_scaled, y_train)"
      ],
      "metadata": {
        "id": "ZoUEnt-pT3t8"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ZoUEnt-pT3t8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking tuned KNN model performance on training set"
      ],
      "metadata": {
        "id": "o9Oit-j5HGlT"
      },
      "id": "o9Oit-j5HGlT"
    },
    {
      "cell_type": "code",
      "source": [
        "knn_tuned_perf_train = model_performance_classification(_______)  ## Complete the code to get model performance on training data\n",
        "knn_tuned_perf_train"
      ],
      "metadata": {
        "id": "kdTEzgneHGld"
      },
      "execution_count": null,
      "outputs": [],
      "id": "kdTEzgneHGld"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to create confusion matrix for training data"
      ],
      "metadata": {
        "id": "ZnN1Ek60HGld"
      },
      "execution_count": null,
      "outputs": [],
      "id": "ZnN1Ek60HGld"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking tuned KNN model performance on test set"
      ],
      "metadata": {
        "id": "mV0OA6weHGld"
      },
      "id": "mV0OA6weHGld"
    },
    {
      "cell_type": "code",
      "source": [
        "knn_tuned_perf_test = model_performance_classification(_______)  ## Complete the code to get model performance on test data\n",
        "knn_tuned_perf_test"
      ],
      "metadata": {
        "id": "B-R-fTKmHGld"
      },
      "execution_count": null,
      "outputs": [],
      "id": "B-R-fTKmHGld"
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to create confusion matrix for test data"
      ],
      "metadata": {
        "id": "yfT-xmstHGle"
      },
      "execution_count": null,
      "outputs": [],
      "id": "yfT-xmstHGle"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree Classifier (pre-pruning)"
      ],
      "metadata": {
        "id": "Y9rTstL4a7n8"
      },
      "id": "Y9rTstL4a7n8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "detailed-possible"
      },
      "source": [
        "### Pre-pruning the tree"
      ],
      "id": "detailed-possible"
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the type of classifier.\n",
        "dt_model_tuned = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Grid of parameters to choose from\n",
        "parameters = {\n",
        "    \"max_depth\": np.arange(5, 13, 2),                          ## Max Depth of the decision tree\n",
        "    \"max_leaf_nodes\": [10, 20, 40, 50, 75, 100],               ## Maximum number of leaf nodes\n",
        "    \"min_samples_split\": [2, 5, 7, 10, 20, 30],                ## Minimum number of samples required to split an internal node\n",
        "    \"class_weight\": ['balanced', None]                         ## whether or not to used balanced weights for impurity computations\n",
        "}\n",
        "\n",
        "# Run the grid search\n",
        "grid_obj = GridSearchCV(dt_model_tuned, parameters, scoring='recall', cv=5)\n",
        "grid_obj = grid_obj.fit(X_train, y_train)\n",
        "\n",
        "# Set the clf to the best combination of parameters\n",
        "dt_model_tuned = grid_obj.best_estimator_\n",
        "\n",
        "# Fit the best algorithm to the data.\n",
        "dt_model_tuned.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "p7tcCFz-Ep8k"
      },
      "id": "p7tcCFz-Ep8k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "endangered-image"
      },
      "source": [
        "### Checking tuned Decision Tree Classifier performance on training set"
      ],
      "id": "endangered-image"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "detected-folks"
      },
      "outputs": [],
      "source": [
        "decision_tree_tuned_perf_train = model_performance_classification(_______)  ## Complete the code to get model performance on training data\n",
        "decision_tree_tuned_perf_train"
      ],
      "id": "detected-folks"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "skilled-poster"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to create confusion matrix for training data"
      ],
      "id": "skilled-poster"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "concrete-season"
      },
      "source": [
        "### Checking tuned Decision Tree Classifier performance on test set"
      ],
      "id": "concrete-season"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "killing-magnet"
      },
      "outputs": [],
      "source": [
        "decision_tree_tuned_perf_test = model_performance_classification(_______)  ## Complete the code to get model performance on test data\n",
        "decision_tree_tuned_perf_test"
      ],
      "id": "killing-magnet"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "banner-comparative"
      },
      "outputs": [],
      "source": [
        "plot_confusion_matrix(_______)  ## Complete the code to create confusion matrix for test data"
      ],
      "id": "banner-comparative"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frequent-grenada"
      },
      "source": [
        "### Visualizing the Decision Tree"
      ],
      "id": "frequent-grenada"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "driving-state"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(20, 10))\n",
        "out = tree.plot_tree(\n",
        "    dt_model_tuned,\n",
        "    feature_names=X_train.columns.tolist(),\n",
        "    filled=True,\n",
        "    fontsize=9,\n",
        "    node_ids=False,\n",
        "    class_names=None,\n",
        ")\n",
        "# below code will add arrows to the decision tree split if they are missing\n",
        "for o in out:\n",
        "    arrow = o.arrow_patch\n",
        "    if arrow is not None:\n",
        "        arrow.set_edgecolor(\"black\")\n",
        "        arrow.set_linewidth(1)\n",
        "plt.show()"
      ],
      "id": "driving-state"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyzing Feature Importance for tuned Decision Tree Classifier"
      ],
      "metadata": {
        "id": "9KvV0HczhLTB"
      },
      "id": "9KvV0HczhLTB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "secure-killing"
      },
      "outputs": [],
      "source": [
        "# Uncomment and run to check feature importance for Tuned Decision Tree model\n",
        "\n",
        "\n",
        "# # importance of features in the tree building\n",
        "\n",
        "# feature_names = X_train.columns.tolist()\n",
        "# importances = dt_model_tuned.feature_importances_\n",
        "# indices = np.argsort(importances)\n",
        "\n",
        "# plt.figure(figsize=(8, 8))\n",
        "# plt.title(\"Feature Importances\")\n",
        "# plt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\n",
        "# plt.yticks(range(len(indices)), [feature_names[i] for i in indices])\n",
        "# plt.xlabel(\"Relative Importance\")\n",
        "# plt.show()"
      ],
      "id": "secure-killing"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Observations from decision tree"
      ],
      "metadata": {
        "id": "glSobMDPZIGy"
      },
      "id": "glSobMDPZIGy"
    },
    {
      "cell_type": "markdown",
      "source": [
        "*  \n"
      ],
      "metadata": {
        "id": "KgAR6S7HaPbF"
      },
      "id": "KgAR6S7HaPbF"
    },
    {
      "cell_type": "markdown",
      "id": "specific-columbus",
      "metadata": {
        "id": "specific-columbus"
      },
      "source": [
        "# **Model Performance Comparison and Final Model Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "superior-reality",
      "metadata": {
        "id": "superior-reality"
      },
      "outputs": [],
      "source": [
        "# training performance comparison\n",
        "\n",
        "models_train_comp_df = pd.concat(\n",
        "    [\n",
        "        logistic_reg_perf_train.T,\n",
        "        logistic_reg_new_perf_train.T,\n",
        "        nb_perf_train.T,\n",
        "        knn_perf_train.T,\n",
        "        knn_tuned_perf_train.T,\n",
        "        decision_tree_perf_train.T,\n",
        "        decision_tree_tuned_perf_train.T\n",
        "            ],\n",
        "    axis=1,\n",
        ")\n",
        "models_train_comp_df.columns = [\n",
        "    \"Logistic Regression Base\",\n",
        "    \"Logistic Regression (Optimal threshold)\",\n",
        "    \"Naive Bayes Base\",\n",
        "    \"KNN Base\",\n",
        "    \"KNN Tuned\",\n",
        "    \"Decision Tree Base\",\n",
        "    \"Decision Tree Tuned\"\n",
        "]\n",
        "print(\"Training performance comparison:\")\n",
        "models_train_comp_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "working-employment",
      "metadata": {
        "id": "working-employment"
      },
      "outputs": [],
      "source": [
        "# testing performance comparison\n",
        "\n",
        "models_test_comp_df = pd.concat(\n",
        "    [\n",
        "        logistic_reg_perf_test.T,\n",
        "        logistic_reg_new_perf_test.T,\n",
        "        nb_perf_test.T,\n",
        "        knn_perf_test.T,\n",
        "        knn_tuned_perf_test.T,\n",
        "        decision_tree_perf_test.T,\n",
        "        decision_tree_tuned_perf_test.T\n",
        "    ],\n",
        "    axis=1,\n",
        ")\n",
        "models_test_comp_df.columns = [\n",
        "    \"Logistic Regression Base\",\n",
        "    \"Logistic Regression (Optimal threshold)\",\n",
        "    \"Naive Bayes Base\",\n",
        "    \"KNN Base\",\n",
        "    \"KNN Tuned\",\n",
        "    \"Decision Tree Base\",\n",
        "    \"Decision Tree Tuned\"\n",
        "]\n",
        "print(\"Test set performance comparison:\")\n",
        "models_test_comp_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "growing-excitement",
      "metadata": {
        "id": "growing-excitement"
      },
      "source": [
        "**Observations**\n",
        "*  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Actionable Insights and Recommendations**"
      ],
      "metadata": {
        "id": "yoNNbiiLJY7i"
      },
      "id": "yoNNbiiLJY7i"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Actionable Insights"
      ],
      "metadata": {
        "id": "BwBFKCLTtiq5"
      },
      "id": "BwBFKCLTtiq5"
    },
    {
      "cell_type": "markdown",
      "id": "otherwise-increase",
      "metadata": {
        "id": "otherwise-increase"
      },
      "source": [
        "\n",
        "*  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Business Recommendations"
      ],
      "metadata": {
        "id": "ErD1O1VQthGo"
      },
      "id": "ErD1O1VQthGo"
    },
    {
      "cell_type": "markdown",
      "id": "marked-coaching",
      "metadata": {
        "id": "marked-coaching"
      },
      "source": [
        "\n",
        "*  "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}